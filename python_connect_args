
####import http.client
import json
import ssl
import argparse

def fetch_data(debug=False):
    # Server details
    host = "example.com"  # Replace with your actual host
    url = "/api/data"  # Replace with the correct endpoint
    port = 443  # HTTPS port

    # Pagination parameters
    limit = 3000
    skip = 0
    all_records = []
    total_records_fetched = 0

    # File to store raw responses
    raw_data_file = "raw_data.txt"

    # Custom SSL context to handle certificate verification and client authentication
    context = ssl.create_default_context(cafile="/path/to/ca-bundle.crt")  # Path to CA bundle
    
    # If client authentication is required, provide the client certificate and key
    context.load_cert_chain(certfile="/path/to/client-cert.pem", keyfile="/path/to/client-key.pem")

    while True:
        # The payload containing the POST data (filters like limit, skip, sort, etc.)
        payload = {
            "limit": limit,
            "skip": skip,
            "sort": {"id": 1}  # Sorting by ID in ascending order
        }
        
        json_payload = json.dumps(payload)

        if debug:
            print(f"Payload: {json_payload}")
        
        try:
            # Create an HTTPS connection to the server
            conn = http.client.HTTPSConnection(host, port, context=context)

            # Set headers for the request
            headers = {
                "Content-Type": "application/json"  # Ensure the Content-Type is JSON
            }

            # Send the POST request with the JSON payload
            conn.request("POST", url, body=json_payload, headers=headers)

            # Get the response from the server
            response = conn.getresponse()

            # Read the raw response
            raw_data = response.read()  # Read as bytes

            # Decode raw data to string
            raw_data_str = raw_data.decode('utf-8')

            # Write raw data to file
            with open(raw_data_file, "a") as file:
                file.write(raw_data_str + "\n")  # Append raw data to the file

            # Check the status code
            if debug:
                print(f"Response Status: {response.status}")
                print(f"Raw Data: {raw_data_str}")

            if response.status == 200:
                try:
                    # Try to parse the JSON data
                    json_data = json.loads(raw_data_str)
                    
                    # Handle response based on its structure
                    if isinstance(json_data, list):
                        # If the response is a list of records
                        num_records = len(json_data)
                        all_records.extend(json_data)
                        total_records_fetched += num_records

                        # If the number of fetched records is less than the limit, stop fetching
                        if num_records < limit:
                            break

                    elif isinstance(json_data, dict):
                        # Handle case where response is a dictionary (e.g., contains metadata)
                        records = json_data.get('records', [])
                        num_records = len(records)
                        all_records.extend(records)
                        total_records_fetched += num_records

                        # Check if more records are available (e.g., based on 'has_more' or 'total' count)
                        has_more = json_data.get('has_more', False)
                        if not has_more:
                            break

                    else:
                        print(f"Unexpected response format: {type(json_data)}")
                        break

                    # Increment the skip parameter for the next batch
                    skip += limit

                except json.JSONDecodeError as json_error:
                    print(f"Error decoding JSON: {json_error}")
                    break

            else:
                print(f"Request failed with status code {response.status}: {raw_data_str}")
                break

        except ssl.SSLError as ssl_error:
            print(f"SSL error occurred: {ssl_error}")
            break
        except Exception as e:
            print(f"An error occurred: {e}")
            break
        finally:
            # Always close the connection
            conn.close()

    # Print or process all fetched records
    print(f"Total records fetched: {total_records_fetched}")
    if debug:
        print("Final Data:", json.dumps(all_records, indent=4))

# Main function to handle argument parsing
if __name__ == "__main__":
    # Argument parser setup
    parser = argparse.ArgumentParser(description="Fetch data with pagination and debugging options.")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode with verbose output")

    # Parse arguments
    args = parser.parse_args()

    # Call the function with the parsed debug argument
    fetch_data(debug=args.debug)

import http.client
import json
import ssl
import argparse
from urllib.parse import urlparse, urljoin

def fetch_data(data_path, debug=False):
    # Define the base URL (root URL)
    base_url = "https://example.com/api/"
    
    # Concatenate base URL with the data path (endpoint)
    full_url = urljoin(base_url, data_path)

    # Parse the full URL into components (scheme, host, path, etc.)
    parsed_url = urlparse(full_url)
    host = parsed_url.hostname
    port = parsed_url.port or 443  # Default to 443 for HTTPS
    path = parsed_url.path

    if debug:
        print(f"Full URL: {full_url}")
        print(f"Connecting to {host} on port {port}, path: {path}")

    # Pagination parameters
    limit = 3000
    skip = 0
    all_records = []
    total_records_fetched = 0

    # File to store raw responses
    raw_data_file = "raw_data.txt"

    # Custom SSL context to handle certificate verification and client authentication
    context = ssl.create_default_context(cafile="/path/to/ca-bundle.crt")  # Path to CA bundle
    
    # If client authentication is required, provide the client certificate and key
    context.load_cert_chain(certfile="/path/to/client-cert.pem", keyfile="/path/to/client-key.pem")

    while True:
        # The payload containing the POST data (filters like limit, skip, sort, etc.)
        payload = {
            "limit": limit,
            "skip": skip,
            "sort": {"id": 1}  # Sorting by ID in ascending order
        }
        
        json_payload = json.dumps(payload)

        if debug:
            print(f"Payload: {json_payload}")
        
        try:
            # Create an HTTPS connection to the server
            conn = http.client.HTTPSConnection(host, port, context=context)

            # Set headers for the request
            headers = {
                "Content-Type": "application/json"  # Ensure the Content-Type is JSON
            }

            # Send the POST request with the JSON payload
            conn.request("POST", path, body=json_payload, headers=headers)

            # Get the response from the server
            response = conn.getresponse()

            # Read the raw response
            raw_data = response.read()  # Read as bytes

            # Decode raw data to string
            raw_data_str = raw_data.decode('utf-8')

            # Write raw data to file
            with open(raw_data_file, "a") as file:
                file.write(raw_data_str + "\n")  # Append raw data to the file

            # Check the status code
            if debug:
                print(f"Response Status: {response.status}")
                print(f"Raw Data: {raw_data_str}")

            if response.status == 200:
                try:
                    # Try to parse the JSON data
                    json_data = json.loads(raw_data_str)
                    
                    # Handle response based on its structure
                    if isinstance(json_data, list):
                        # If the response is a list of records
                        num_records = len(json_data)
                        all_records.extend(json_data)
                        total_records_fetched += num_records

                        # If the number of fetched records is less than the limit, stop fetching
                        if num_records < limit:
                            break

                    elif isinstance(json_data, dict):
                        # Handle case where response is a dictionary (e.g., contains metadata)
                        records = json_data.get('records', [])
                        num_records = len(records)
                        all_records.extend(records)
                        total_records_fetched += num_records

                        # Check if more records are available (e.g., based on 'has_more' or 'total' count)
                        has_more = json_data.get('has_more', False)
                        if not has_more:
                            break

                    else:
                        print(f"Unexpected response format: {type(json_data)}")
                        break

                    # Increment the skip parameter for the next batch
                    skip += limit

                except json.JSONDecodeError as json_error:
                    print(f"Error decoding JSON: {json_error}")
                    break

            else:
                print(f"Request failed with status code {response.status}: {raw_data_str}")
                break

        except ssl.SSLError as ssl_error:
            print(f"SSL error occurred: {ssl_error}")
            break
        except Exception as e:
            print(f"An error occurred: {e}")
            break
        finally:
            # Always close the connection
            conn.close()

    # Print or process all fetched records
    print(f"Total records fetched: {total_records_fetched}")
    if debug:
        print("Final Data:", json.dumps(all_records, indent=4))

# Main function to handle argument parsing
if __name__ == "__main__":
    # Argument parser setup
    parser = argparse.ArgumentParser(description="Fetch data from the API using a data path and debugging options.")
    
    # Positional argument for the data path (endpoint)
    parser.add_argument("data", help="The API data path (endpoint) to fetch data from (e.g., 'data' for '/api/data')")
    
    # Optional debug flag
    parser.add_argument("--debug", action="store_true", help="Enable debug mode with verbose output")

    # Parse arguments
    args = parser.parse_args()

    # Call the function with the parsed arguments
    fetch_data(data_path=args.data, debug=args.debug)

python script.py users --debug

# Main function to handle argument parsing
if __name__ == "__main__":
    # Argument parser setup
    parser = argparse.ArgumentParser(description="Fetch data from the API using a data path and debugging options.")
    
    # Optional argument for the data path (endpoint) with a default value
    parser.add_argument(
        "--data", 
        default="data",  # Default value for the data path
        help="The API data path (endpoint) to fetch data from (default: 'data' for '/api/data')"
    )
    
    # Optional debug flag
    parser.add_argument("--debug", action="store_true", help="Enable debug mode with verbose output")

    # Parse arguments
    args = parser.parse_args()

    # Call the function with the parsed arguments
    fetch_data(data_path=args.data, debug=args.debug)

#### mkdir 
import errno
import os
from datetime import datetime

def filecreation(list, filename):
    mydir = os.path.join(
        os.getcwd(), 
        datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))
    try:
        os.makedirs(mydir)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise  # This was not a "directory exist" error..
    with open(os.path.join(mydir, filename), 'w') as d:
        d.writelines(list)
## or
# using with statement
with open('file_path', 'w') as file:
    file.write('hello world !')

### scheduler
import schedule
import time

# Define a function to print a message


def print_message():
    print("Hello! It's time to code on GFG")


# Schedule the task to run every day at 7:00 AM
schedule.every().day.at("10:35").do(print_message)

while True:
    schedule.run_pending()
    time.sleep(1)


## or
import schedule
import time


def print_message():
    print("Task executed at:", time.strftime("%H:%M:%S"))


# Schedule task to run every 5 seconds
schedule.every(5).seconds.do(print_message)

# Keep the program running to allow scheduled tasks to execute
while True:
    schedule.run_pending()
    time.sleep(1)

####
import datetime as dt

def safe_convert(x):
    try:
        return dt.datetime.fromtimestamp(x / 1000) if isinstance(x, int) else x
    except (TypeError, ValueError):
        return x

data['created'] = data['created'].apply(safe_convert)
####
import datetime as dt
import pandas as pd
from pandas import json_normalize

# Example usage if loading JSON data
# Assuming each line in the file is a separate JSON object
with open('file.json', 'r') as f:
    data = pd.DataFrame([json_normalize(json.loads(line)) for line in f])

def safe_convert(x):
    # Check if x is None or not an integer
    if x is None:
        return None
    elif isinstance(x, int):
        try:
            # Attempt to convert the timestamp
            return dt.datetime.fromtimestamp(x / 1000)
        except (TypeError, ValueError):
            # Return None if there's an error during conversion
            return None
    else:
        # Return the original value if it's not an integer
        return x

# Apply the conversion function safely
data['created'] = data['created'].apply(safe_convert)

import datetime as dt
import pandas as pd
from pandas import json_normalize

# Example usage if loading JSON data
# Assuming each line in the file is a separate JSON object
with open('file.json', 'r') as f:
    data = pd.DataFrame([json_normalize(json.loads(line)) for line in f])

def safe_convert(x):
    # Check if x is None or not an integer
    if x is None:
        return None
    elif isinstance(x, int):
        try:
            # Attempt to convert the timestamp
            return dt.datetime.fromtimestamp(x / 1000)
        except (TypeError, ValueError):
            # Return None if there's an error during conversion
            return None
    else:
        # Return the original value if it's not an integer
        return x

# Apply the conversion function safely
data['created'] = data['created'].apply(safe_convert)

####
import datetime as dt
import pandas as pd
from pandas import json_normalize
import json

# Load data from JSON file while handling empty lines
def load_json_data(file_path):
    with open(file_path, 'r') as f:
        data = []
        for line in f:
            line = line.strip()  # Remove leading and trailing whitespace
            if line:  # Ensure the line is not empty
                try:
                    json_data = json.loads(line)
                    data.append(json_normalize(json_data))
                except json.JSONDecodeError:
                    print(f"Skipping invalid JSON line: {line}")
        return pd.concat(data, ignore_index=True)

# Load the data
data = load_json_data('file.json')

# Conversion function to safely handle the 'created' column
def safe_convert(x):
    if x is None:
        return None
    elif isinstance(x, int):
        try:
            return dt.datetime.fromtimestamp(x / 1000)
        except (TypeError, ValueError):
            return None
    else:
        return x

# Apply the conversion function safely
data['created'] = data['created'].apply(safe_convert)


###
import json

data_to_export = [...]  # Your list of dictionaries to export

with open('output.json', 'w') as f:
    for i, item in enumerate(data_to_export):
        json.dump(item, f)
        if i < len(data_to_export) - 1:
            f.write('\n')  # Write newline only if it's not the last item

import datetime as dt

data['created'] = data['created'].apply(
    lambda x: dt.datetime.fromtimestamp(x / 1000) if isinstance(x, int) else x
)
